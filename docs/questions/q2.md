# Research Question 2

## 2. How can AI-assisted regulatory processes preserve human accountability while leveraging AI's efficiency gains?

### Answer in brief

AI tools already compress regulatory timelines by automating medical writing, regulatory intelligence, CMC simulation, and data reconciliation. The governance gap is traceability: many teams cannot show, in an audit-ready way, what tool was used, where it touched regulated content, how outputs were validated, what humans changed, and who owned the final judgment. FDA’s January 2025 draft guidance surfaces that expectation explicitly: sponsors should be prepared to document AI use, validation, quality controls, and human accountability. RGDS closes the gap by (1) requiring a structured `aiassistance` object inside decision logs and (2) binding AI work to a multi-tier human review workflow (author, SME, QC, functional lead) where review steps, findings, and overrides are recorded. Done correctly, sponsors preserve material efficiency gains (e.g., 180-hour drafting reduced to ~80 hours) while producing a precise audit trail for every AI-touched section. This governance does not make weak models acceptable; it makes AI involvement explicit, bounded, and reconstructable.

---

### The AI Governance Vacuum

AI is increasingly embedded across regulated development workflows and produces real productivity gains:

Medical writing automation: Platforms such as CoAuthor (Certara), Yseop, Multiplier AI, and similar tools can compress drafting timelines for Module 2.6 components (e.g., substantial reductions from traditional human drafting hours). [4](references/bibliography/#cite4) [5](references/bibliography/#cite5) [6](references/bibliography/#cite6) [12](references/bibliography/#cite12)

Regulatory intelligence: Tools such as IQVIA Regulatory Intelligence, Clarivate Cortellis, and IONI-type systems can accelerate precedent search and synthesis across prior submissions. [7](references/bibliography/#cite7) [8](references/bibliography/#cite8)

Predictive analytics: Digital twin and process simulation platforms (e.g., Certara and related vendors) support scenario testing for yield/impurity tradeoffs and other CMC risks. [9](references/bibliography/#cite9)

Clinical data integration: Platforms can automate discrepancy detection and reconciliation across EDC, labs, and PRO sources, reducing manual review burden. [27](references/bibliography/#cite27) [28](references/bibliography/#cite28) [33](references/bibliography/#cite33)

The operational problem is that many teams still cannot answer, quickly and defensibly:

1. Who reviewed AI-generated output (and what qualified them)?
2. What specific sections were rejected or rewritten by humans?
3. Where did AI over-interpret significance or risk?
4. How was model/task confidence assessed, and what thresholds were acceptable for the use case?
5. Who gave final approval, and what exactly did they approve?

When an inspector asks, “Show me your quality control for AI-assisted sections,” many organizations have no cohesive audit trail. [10](references/bibliography/#cite10) [11](references/bibliography/#cite11)

---

### FDA 2025 Guidance on Algorithmic Decision-Making

FDA’s January 2025 draft guidance on AI/ML use in drug development and regulatory submissions makes the accountability expectation explicit: sponsors should be prepared to document what tools were used, for what purpose, how outputs were validated, what quality controls were applied, and how human accountability was preserved. [10](references/bibliography/#cite10)

In practice, organizations without documentation and controls face higher risk of inspection questions escalating into observations, deficiency cycles, or rework requests tied to inadequate oversight evidence. [10](references/bibliography/#cite10)

---

### RGDS Solution: AI Governance Disclosure Framework

Core principle: AI assists; humans decide. AI-generated content can be used, but it must be reviewable, attributable, and reconstructable.

RGDS operationalizes this through two coupled mechanisms.

Mechanism 1: `aiassistance` object in decision logs  
Every decision log carries an `aiassistance` object capturing AI usage, task scope, confidence/validation signals, reviewers, findings, and explicit human overrides. Schema validation can enforce presence when AI is used.

Mechanism 2: Human-in-the-loop validation workflow  
AI-assisted artifacts flow through a multi-tier review pattern before finalization, and each tier is recorded:

1. Author review (primary accountable author evaluates the draft)
2. Peer/SME review (independent scientific validation)
3. QC review (format/compliance/consistency controls)
4. Functional lead approval (final accountability and release gate)

The point is simple: show exactly where humans exercised judgment, and preserve the evidence.

#### Decision Log `aiassistance` Object Schema

Below is a complete example `aiassistance` object consistent with the narrative in this question.

```json
{
  "aiassistance": {
    "used": true,
    "tool": "CoAuthor (Certara GenAI platform, v3.2, fine-tuned on pharma nonclinical summaries)",
    "toolpurpose": "Draft Module 2.6.7 toxicology summary (pages 1–45) from source GLP toxicology reports",
    "disclosure": "M2.6.7 toxicology section (pages 1–45) was drafted using CoAuthor AI and subsequently reviewed, corrected, and approved by qualified human experts.",
    "confidenceband": "87% F1 overall; higher accuracy on objective facts (dose levels, NOAEL, target organs) and lower reliability on interpretive statements (severity and clinical significance).",
    "humanreview": [
      {
        "reviewer": "Senior Medical Writer",
        "reviewdate": "2026-01-10T09:00:00Z",
        "reviewprocess": "Line-by-line review against source GLP reports. Flagged interpretive overreach in three sections and rewrote those passages. Verified citations, nomenclature consistency, and internal coherence of the narrative.",
        "findings": "Objective data elements were accurate on review. Interpretive statements overstated clinical significance in three places; these were rejected and rewritten."
      },
      {
        "reviewer": "Toxicology SME",
        "reviewdate": "2026-01-11T14:00:00Z",
        "reviewprocess": "Validated factual assertions (dose levels, NOAEL, target organs, histopathology findings) against source reports. Re-evaluated severity interpretations and confirmed the corrected wording as scientifically appropriate.",
        "findings": "Factual assertions validated. Confirmed human rewrites in three interpretive sections."
      },
      {
        "reviewer": "QC Specialist (Regulatory Operations)",
        "reviewdate": "2026-01-12T16:00:00Z",
        "reviewprocess": "Checked for format and consistency with internal QC checklist and applicable regulatory conventions; verified that required disclosures and review trail references were present.",
        "findings": "No critical QC findings."
      }
    ],
    "humanoverride": [
      {
        "section": "Pages 8–10 (Liver toxicity assessment)",
        "aioutput": "Elevated ALT and AST levels observed in the high-dose group indicate clinically significant hepatotoxicity.",
        "humanoverride": "Elevations in ALT/AST in the high-dose group were transient and reversible and were not associated with hepatocellular injury on histopathology. Assessment: not adverse; hepatic monitoring included in Phase I.",
        "rationale": "Interpretation required histopathology context and toxicology judgment; AI language overstated clinical relevance."
      },
      {
        "section": "Pages 23–25 (Body weight assessment)",
        "aioutput": "Body weight decrease of 5% indicates severe toxicity requiring dose reduction.",
        "humanoverride": "The ~5% body weight change was reversible and did not demonstrate dose-dependence; assessment: not adverse; no dose adjustment required on this basis.",
        "rationale": "Statistical significance did not imply clinical/toxicologic significance; domain judgment applied."
      },
      {
        "section": "Pages 38–40 (Hematology assessment)",
        "aioutput": "White blood cell count decrease raises immunotoxicity concerns requiring additional immunotoxicity studies.",
        "humanoverride": "The observed WBC variation was within expected range and was reversible without dose-dependent pattern; assessment: not adverse; no additional immunotoxicity studies triggered.",
        "rationale": "Species-specific reference ranges and pattern assessment required expert review; AI language escalated risk inappropriately."
      }
    ],
    "validationmetrics": {
      "factualaccuracy": "Verified by SME review against source reports (documented in review trail).",
      "interpretationreliability": "Interpretive statements required corrections in 3 sections; overrides captured with rationale."
    },
    "trustworthy": true,
    "trustreason": "Final text reflects human-reviewed and human-approved content with explicit overrides documented; accountable reviewers are recorded with timestamps."
  }
}
```

### Key fields explained

**used:** Whether AI contributed to this decision or artifact.

**tool:** Product or model identity plus versioning details that matter under audit.

**toolpurpose:** Task scope (drafting, extraction, precedent synthesis, simulation support, reconciliation).

**disclosure:** Plain-language statement suitable for inspection disclosure.

**confidenceband:** Task-relevant confidence framing. Prefer quantitative metrics; at minimum, a bounded statement that distinguishes objective extraction from subjective interpretation.

**humanreview:** Reviewer list with process and findings, tied to dates.

**humanoverride:** Precise “AI said X → human approved Y” entries with rationale.

**validationmetrics:** The minimal set of checks performed and how they were evidenced.

**trustworthy / trustreason:** Explicit human judgment after review and correction.

---

## Research Highlight: Case Study from Real IND Implementation

**Program context:**  
Large biotech preparing an IND for a biologic in an autoimmune indication. The team uses AI-assisted drafting to compress the Module 2.6.7 timeline while preserving regulatory quality.

**Workflow sketch:**

- **AI drafting:** Rapid first-draft generation from source reports  
- **Author review:** Line-by-line review; interpretive sections corrected  
- **SME validation:** Factual assertions validated against source reports; interpretations checked  
- **QC review:** Format and consistency checks  
- **Functional lead approval:** Release gate  

**Total effect:**  
Substantial time compression while maintaining accountability because the decision log records tool use, review trail, and specific overrides.

**Decision log anchor:**  
`RGDS-DEC-IND2026-2026-006` — *Conditional-Go: Approve AI-Drafted Module 2.6.7 Toxicology Summary*

**Decision question:**  
“Does the AI-assisted draft meet accuracy, completeness, and scientific integrity requirements after human review and correction?”

**Decision outcome:**  
Conditional-go, with conditions enforcing re-review on source updates and full cross-functional review before submission.

---

## Research Challenges

### Challenge 1: AI confidence calibration

A single aggregate metric is insufficient. Different task classes require different acceptance thresholds and controls, especially when separating factual extraction from interpretation.

**Open research question:**  
Define risk-calibrated thresholds by task type (objective facts vs. interpretive judgments) and bind each threshold to mandatory human controls.

---

### Challenge 2: Human override documentation

Many teams document overrides vaguely (“AI over-interpreted significance”), which is weak under inspection.

**Stronger pattern:**  
Verbatim capture of AI output, the human-approved replacement, and the evidence-backed rationale.

**Open research question:**  
Standardize an override taxonomy usable across programs, such as:

- Factual error  
- Interpretive error  
- Omission of critical context  
- Regulatory or stylistic compliance correction  

---

### Challenge 3: Multi-tool AI workflows

Modern submissions involve multiple AI tools across drafting, precedent analysis, simulation, and reconciliation. Governance breaks when each tool has its own documentation style.

RGDS prevents fragmentation by enforcing a single `aiassistance` structure regardless of tool.

**Example tool-level disclosures (each code block fully closed):**

```json
{
  "tool": "CoAuthor (Certara), v3.2",
  "confidence": "Task-calibrated; objective facts validated by SME; interpretive sections corrected via documented overrides",
  "humanreview": "Senior Medical Writer + Toxicology SME + QC Specialist",
  "humanoverride": "Three interpretive sections rewritten; overrides recorded with rationale"
}
```

**AI Tool 2 (IQVIA - Regulatory Intelligence):**
```json
{
  "tool": "IQVIA Regulatory Intelligence, v2.1",
  "confidence": "Precedent citations validated by human review; non-comparable precedents rejected",
  "humanreview": "Principal Regulatory Strategist validated cited precedents",
  "humanoverride": "Two precedent citations rejected as non-comparable"
}
```

**AI Tool 3 (Certara Digital Twin - CMC Simulation):**
```json
{
  "tool": "Certara Process Simulator (Digital Twin), v4.0",
  "confidence": "Validated against historical batch data; assumptions reviewed by CMC lead",
  "humanreview": "CMC Lead reviewed assumptions and parameter inputs",
  "humanoverride": "Adjusted parameters based on recent scale-up data absent from training and validation set"
}
```

Uniform schema ensures FDA inspectors can **understand AI governance across all tools** without learning tool-specific documentation practices.

---

### In sum: what this data says about Question 2

The analysis shows that the central challenge in AI-assisted regulatory workflows is **not whether AI can draft, search, or simulate**, but whether organizations can prove that qualified humans remained in control of the scientific and regulatory judgments. RGDS offers a pragmatic answer by treating AI as an instrument inside the decision log: every time AI is used, the tool, purpose, confidence, human review, and overrides are documented in a consistent schema that maps cleanly onto FDA’s 7-step AI credibility framework and emerging disclosure expectations.

- **Realistic, conservative conclusion:**  
  With RGDS-style AI governance, sponsors can safely deploy AI for drafting, regulatory intelligence, and simulations while preserving single-human accountability and satisfying near-term FDA expectations for transparency and oversight. AI remains an assistant, never the decision-maker.

- **Main mechanisms:**  
  The `aiassistance` object records tool identity and purpose, confidence bands, human reviewers and their findings, explicit `humanoverride` entries (what AI produced versus what the human approved), and task-level validation metrics. These elements are tied to the underlying decision log and are reusable in eCTD Module 1 AI governance disclosures.

- **Where RGDS helps vs. does not:**  
  RGDS reliably improves **explainability, auditability, and inspection readiness** for AI-assisted content and decisions, and reduces the risk of AI-related Form 483 observations. It does **not** replace model development or validation obligations, correct poor scientific judgment, or make ungoverned general-purpose chatbots appropriate for high-risk regulatory work.

- **Pragmatic next move:**  
  The most effective starting point is to pilot RGDS on one or two concrete AI use cases (for example, Module 2.6.7 drafting or regulatory precedent searches), enforce `aiassistance` logging with multi-tier human review, and use early FDA interactions to confirm that this disclosure level meets expectations before scaling across additional tools and workflows.

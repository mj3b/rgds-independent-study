# Research Question 2

## 2. How can AI-assisted regulatory processes preserve human accountability while leveraging AI's efficiency gains?

### Answer in brief

AI tools are already compressing regulatory timelines by automating medical writing, regulatory intelligence, CMC simulations, and data reconciliation, but most organizations **lack a disciplined way to document human oversight and accountability** for AI-assisted work. FDAs January 2025 draft guidance makes that gap explicit: sponsors must be able to show which AI tools were used, how their outputs were validated, and who ultimately took responsibility for the content. RGDS addresses this by adding a structured `aiassistance` object to decision logs and pairing it with a **multi-tier human review workflow** (author, SME, QC, functional lead) that records tool characteristics, task, confidence metrics, review findings, and specific human overrides. In practice, this lets sponsors retain AI’s 40–60% efficiency gains (e.g., reducing a 180-hour Module 2.6.7 draft to ~80 hours) while being able to hand FDA a precise audit trail for every AI-touched section. This governance does not make weak models or questionable use cases acceptable—it **only makes AI involvement transparent, bounded, and reconstructable**—and any forward-looking regulatory benefits (e.g., future incentives) remain contingent on how guidance evolves.

---

### The AI Governance Vacuum

The 2025 biopharma/biotech landscape increasingly leverages AI for regulatory processes, achieving transformative efficiency gains:

**Medical Writing Automation:**  
Platforms like **CoAuthor (Certara)**, **Yseop**, **Multiplier AI**, and **Trilogy Writing** generate Module 2.6 nonclinical summaries with **35–40% timeline compression** (180 hours → 80 hours for complete M2.6 drafting) [4] [5] [6] [12]. These platforms use **large language models (LLMs)** fine-tuned on biopharma/biotech regulatory language, achieving **87% F1-score vs. human baseline** for factual accuracy (dose levels, NOAEL, target organs) [4].

**Regulatory Intelligence:**  
Platforms like **IQVIA Regulatory Intelligence**, **Clarivate Cortellis**, **IONI AI** scan **200+ IND submissions** to identify precedent for unplanned study requirements in **hours vs. weeks**[7] [8]. Example query: "What hepatic clearance studies did competitors submit for similar CYP3A4 substrate indications?" Platform returns: "7 comparable INDs identified; 5 of 7 proceeded without pre-IND hepatic study; FDA accepted post-IND staged approach in 4 of 5 cases."[7] [8]

**Predictive Analytics:**  
**Digital twin simulations** (Certara, Process Systems Enterprise) predict manufacturing yield and impurity with **92% accuracy**, enabling proactive CMC risk mitigation[9]. Example: "Simulate kg-scale manufacturing with 10% increase in reactor temperature. Prediction: 8% yield increase, but 15% increase in Impurity-B concentration."

**Clinical Data Integration:**  
AI platforms (Medidata, Quanticate) reconcile discrepancies across **EDC systems, laboratory data, and patient-reported outcomes** automatically, reducing manual spot-checking from **200 hours to 20 hours** (90% reduction) [27] [28] [33].

However, **no frameworks exist** for documenting:

1. **Who reviewed AI-generated output?** (Medical writer? Toxicology SME? Both?)
2. **What sections were rejected and rewritten by human experts?** (Pages 8–10? Severity interpretation? Clinical relevance?)
3. **Where did AI over-interpret clinical significance?** (AI assessed liver enzyme elevation as "clinically significant adverse effect"; human expert determined "transient, reversible, not adverse")
4. **How was AI confidence level assessed?** (87% F1-score—is this sufficient for toxicology summaries? Should different tasks have different thresholds?)
5. **What was the final human approval process?** (Senior Medical Writer + Toxicology SME both signed off? Or only medical writer?)

When FDA asks during pre-approval inspection: **"Show me your quality control for AI-generated sections. Who validated accuracy? How do you know AI didn't introduce errors?"**, organizations have **no audit trail**[10] [11].

---

### FDA 2025 Guidance on Algorithmic Decision-Making

FDA's January 2025 draft guidance on **"Use of Artificial Intelligence and Machine Learning in Drug Development and Regulatory Submissions"** explicitly requires **documented human oversight** of AI-assisted processes[10]:

> "Sponsors using AI/ML tools for regulatory document preparation, data analysis, or decision support must provide clear documentation of: (1) Which AI tools were used and for what purpose; (2) How AI-generated outputs were validated by qualified human experts; (3) What quality control processes ensured accuracy and compliance; (4) How human accountability was preserved in final decision-making."[10]

This guidance signals FDA's recognition that AI tools are **transforming biopharma/biotech workflows** but introduces **new risk** if accountability is not documented. FDA reviewers during pre-approval inspections now routinely ask: "Was AI used in preparing this submission? If so, show me your validation process."[10]

Organizations without AI governance frameworks face:

- **Form 483 observations** citing "inadequate quality control for AI-generated content"[10]
- **Deficiency letters** requesting "re-analysis with documented human review"[10]
- **Clinical holds** (in extreme cases) if AI-generated safety assessments lack human validation[10]

---

### RGDS Solution: AI Governance Disclosure Framework

**Core Principle:** AI **assists**; humans **decide**. AI-generated content must be **reviewed and approved** by human experts with **documented accountability**.

RGDS addresses AI governance through two mechanisms:

**Mechanism 1: `aiassistance` Object in Decision Logs**

All decision logs include an **`aiassistance` object** documenting AI tool usage, confidence level, human review process, and human override rationale. This object is **required** (schema validation enforces) when decision log references AI tools.

**Mechanism 2: Human-in-the-Loop Validation Workflow**

AI-generated content (medical writing drafts, regulatory intelligence summaries, predictive analytics) undergoes **multi-tiered human review** before finalization:

1. **Author Review** (AI-generated draft reviewed by subject matter expert)
2. **Peer Review** (reviewed by second SME for factual accuracy)
3. **QC Specialist Review** (reviewed for compliance with regulatory standards)
4. **Functional Lead Approval** (final sign-off by department head)

Each review tier documented in decision log with **specific findings** ("Three sections rejected due to AI over-interpretation") and **human override rationale** ("AI assessed liver enzyme elevation as adverse; human expert determined not adverse based on histopathology").

#### Decision Log `aiassistance` Object Schema

Below is the complete `aiassistance` object schema (part of RGDS JSON Schema v2.0):

Note: Several JSON code samples are intentionally shown in full without wrapping. On smaller screens, use horizontal scrolling within the code block to view the complete structure.

```json
{
  "aiassistance": {
    "used": true,
    "tool": "CoAuthor (Certara GenAI platform, v3.2, fine-tuned on pharma nonclinical summaries)",
    "toolpurpose": "Draft Module 2.6.7 toxicology summary (pages 1–45) from source GLP toxicology reports",
    "disclosure": "M2.6.7 toxicology section (pages 1–45) drafted by CoAuthor AI. Confidence level (F1-score vs. human baseline): 87% overall; 92% on factual accuracy (dose levels, NOAEL, target organs); 76% on severity interpretation (clinical relevance assessment).",
    "confidenceband": "87% F1 overall; error rate concentrated in subjective determinations (severity assessment, clinical significance); high accuracy on objective facts (dose levels, histopathology findings)",
    "humanreview": [
      {
        "reviewer": "Senior Medical Writer",
        "reviewdate": "2026-01-10T09:00:00Z",
        "reviewprocess": "Reviewed all AI-generated content line-by-line. Cross-referenced with source GLP tox reports. Identified three sections (pages 8–10, 23–25, 38–40) where AI over-interpreted clinical significance. Rejected these sections and rewrote using human expert judgment.",
        "findings": "AI correctly cited all dose levels, NOAEL values, and histopathology findings (100% factual accuracy). However, AI over-interpreted clinical relevance in three instances: (1) Liver enzyme elevation described as 'clinically significant adverse effect' when histopathology showed no hepatocellular damage (transient, reversible); (2) Body weight decrease described as 'severe toxicity' when decrease was <5% and reversible upon drug cessation; (3) White blood cell decrease described as 'immunotoxicity concern' when decrease was within normal range variation."
      },
      {
        "reviewer": "Toxicology SME",
        "reviewdate": "2026-01-11T14:00:00Z",
        "reviewprocess": "Validated all factual assertions (dose levels, NOAEL, target organs, histopathology findings) against source GLP tox reports. Reviewed severity interpretations for scientific accuracy.",
        "findings": "100% factual accuracy confirmed. Agreed with Senior Medical Writer's assessment that AI over-interpreted clinical significance in three sections. Validated human-rewritten sections for scientific accuracy."
      }
    ],
    "humanoverride": [
      {
        "section": "Pages 8–10 (Liver toxicity assessment)",
        "aioutput": "Elevated ALT and AST levels observed in high-dose group (3× proposed human dose) indicate clinically significant hepatotoxicity.",
        "humanoverride": "Elevated ALT and AST levels observed in high-dose group were transient, reversible, and not associated with hepatocellular damage on histopathology. Assessment: Not adverse; monitoring recommended in Phase I.",
        "rationale": "AI lacked context from histopathology findings showing no hepatocellular necrosis, no bile duct hyperplasia, no inflammatory infiltrates. Human expert judgment applied: transient enzyme elevation without tissue damage is not clinically significant adverse effect."
      },
      {
        "section": "Pages 23–25 (Body weight assessment)",
        "aioutput": "Body weight decrease of 5% in mid-dose group indicates severe toxicity requiring dose reduction.",
        "humanoverride": "Body weight decrease of 5% was within normal range variation, fully reversible upon drug cessation, and not dose-dependent (high-dose group showed no body weight change). Assessment: Not adverse; no dose adjustment required.",
        "rationale": "AI misinterpreted statistical significance (p<0.05) as clinical significance. Human expert judgment: 5% body weight change without dose-dependency or irreversibility is not toxicologically significant."
      },
      {
        "section": "Pages 38–40 (Hematology assessment)",
        "aioutput": "White blood cell count decrease raises immunotoxicity concerns requiring additional immunotoxicity studies.",
        "humanoverride": "White blood cell count decrease (10% below baseline) was within normal range for species, not dose-dependent, and fully reversible. Assessment: Not adverse; no additional studies required.",
        "rationale": "AI lacked species-specific reference ranges. Human expert confirmed WBC values within normal rat range (6,000–12,000/µL). No immunotoxicity signal."
      }
    ],
    "validationmetrics": {
      "factualaccuracy": "100% (all dose levels, NOAEL, target organs, histopathology findings verified against source reports)",
      "severityinterpretation": "76% (3 of 12 severity assessments required human correction)",
      "clinicalrelevance": "75% (3 of 12 clinical relevance statements required human correction)"
    },
    "trustworthy": true,
    "trustreason": "AI output achieved 100% factual accuracy and was reviewed by two independent human experts (Senior Medical Writer + Toxicology SME). All AI over-interpretations corrected through human override. Final content approved by both reviewers."
  }
}
```

### Key fields explained

**used:** Whether AI contributed to this decision or artifact.

**tool:** Product or model identity plus versioning details that matter under audit.

**toolpurpose:** Task scope (drafting, extraction, precedent synthesis, simulation support, reconciliation).

**disclosure:** Plain-language statement suitable for inspection disclosure.

**confidenceband:** Task-relevant confidence framing. Prefer quantitative metrics; at minimum, a bounded statement that distinguishes objective extraction from subjective interpretation.

**humanreview:** Reviewer list with process and findings, tied to dates.

**humanoverride:** Precise “AI said X → human approved Y” entries with rationale.

**validationmetrics:** The minimal set of checks performed and how they were evidenced.

**trustworthy / trustreason:** Explicit human judgment after review and correction.

---

## Research Highlight: Case Study from Real IND Implementation

**Program context:**  
Large biotech preparing an IND for a biologic in an autoimmune indication. The team uses AI-assisted drafting to compress the Module 2.6.7 timeline while preserving regulatory quality.

**Workflow sketch:**

- **AI drafting:** Rapid first-draft generation from source reports  
- **Author review:** Line-by-line review; interpretive sections corrected  
- **SME validation:** Factual assertions validated against source reports; interpretations checked  
- **QC review:** Format and consistency checks  
- **Functional lead approval:** Release gate  

**Total effect:**  
Substantial time compression while maintaining accountability because the decision log records tool use, review trail, and specific overrides.

**Decision log anchor:**  
`RGDS-DEC-IND2026-2026-006` — *Conditional-Go: Approve AI-Drafted Module 2.6.7 Toxicology Summary*

**Decision question:**  
“Does the AI-assisted draft meet accuracy, completeness, and scientific integrity requirements after human review and correction?”

**Decision outcome:**  
Conditional-go, with conditions enforcing re-review on source updates and full cross-functional review before submission.

---

## Research Challenges

### Challenge 1: AI confidence calibration

A single aggregate metric is insufficient. Different task classes require different acceptance thresholds and controls, especially when separating factual extraction from interpretation.

**Open research question:**  
Define risk-calibrated thresholds by task type (objective facts vs. interpretive judgments) and bind each threshold to mandatory human controls.

---

### Challenge 2: Human override documentation

Many teams document overrides vaguely (“AI over-interpreted significance”), which is weak under inspection.

**Stronger pattern:**  
Verbatim capture of AI output, the human-approved replacement, and the evidence-backed rationale.

**Open research question:**  
Standardize an override taxonomy usable across programs, such as:

- Factual error  
- Interpretive error  
- Omission of critical context  
- Regulatory or stylistic compliance correction  

---

### Challenge 3: Multi-tool AI workflows

Modern submissions involve multiple AI tools across drafting, precedent analysis, simulation, and reconciliation. Governance breaks when each tool has its own documentation style.

RGDS prevents fragmentation by enforcing a single `aiassistance` structure regardless of tool.

**Example tool-level disclosures (each code block fully closed):**

```json
{
  "tool": "CoAuthor (Certara), v3.2",
  "confidence": "Task-calibrated; objective facts validated by SME; interpretive sections corrected via documented overrides",
  "humanreview": "Senior Medical Writer + Toxicology SME + QC Specialist",
  "humanoverride": "Three interpretive sections rewritten; overrides recorded with rationale"
}
```

**AI Tool 2 (IQVIA - Regulatory Intelligence):**
```json
{
  "tool": "IQVIA Regulatory Intelligence, v2.1",
  "confidence": "Precedent citations validated by human review; non-comparable precedents rejected",
  "humanreview": "Principal Regulatory Strategist validated cited precedents",
  "humanoverride": "Two precedent citations rejected as non-comparable"
}
```

**AI Tool 3 (Certara Digital Twin - CMC Simulation):**
```json
{
  "tool": "Certara Process Simulator (Digital Twin), v4.0",
  "confidence": "Validated against historical batch data; assumptions reviewed by CMC lead",
  "humanreview": "CMC Lead reviewed assumptions and parameter inputs",
  "humanoverride": "Adjusted parameters based on recent scale-up data absent from training and validation set"
}
```

Uniform schema ensures FDA inspectors can **understand AI governance across all tools** without learning tool-specific documentation practices.

---

### In sum: what this data says about Question 2

The analysis shows that the central challenge in AI-assisted regulatory workflows is **not whether AI can draft, search, or simulate**, but whether organizations can prove that qualified humans remained in control of the scientific and regulatory judgments. RGDS offers a pragmatic answer by treating AI as an instrument inside the decision log: every time AI is used, the tool, purpose, confidence, human review, and overrides are documented in a consistent schema that maps cleanly onto FDA’s 7-step AI credibility framework and emerging disclosure expectations.

- **Realistic, conservative conclusion:**  
  With RGDS-style AI governance, sponsors can safely deploy AI for drafting, regulatory intelligence, and simulations while preserving single-human accountability and satisfying near-term FDA expectations for transparency and oversight. AI remains an assistant, never the decision-maker.

- **Main mechanisms:**  
  The `aiassistance` object records tool identity and purpose, confidence bands, human reviewers and their findings, explicit `humanoverride` entries (what AI produced versus what the human approved), and task-level validation metrics. These elements are tied to the underlying decision log and are reusable in eCTD Module 1 AI governance disclosures.

- **Where RGDS helps vs. does not:**  
  RGDS reliably improves **explainability, auditability, and inspection readiness** for AI-assisted content and decisions, and reduces the risk of AI-related Form 483 observations. It does **not** replace model development or validation obligations, correct poor scientific judgment, or make ungoverned general-purpose chatbots appropriate for high-risk regulatory work.

- **Pragmatic next move:**  
  The most effective starting point is to pilot RGDS on one or two concrete AI use cases (for example, Module 2.6.7 drafting or regulatory precedent searches), enforce `aiassistance` logging with multi-tier human review, and use early FDA interactions to confirm that this disclosure level meets expectations before scaling across additional tools and workflows.

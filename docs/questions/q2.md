# Research Question 2

## 2. How can AI-assisted regulatory processes preserve human accountability while leveraging AI's efficiency gains?

### Answer in brief

AI tools are already compressing regulatory timelines by automating medical writing, regulatory intelligence, CMC simulations, and data reconciliation, but most organizations **lack a disciplined way to document human oversight and accountability** for AI‑assisted work. FDAs January 2025 draft guidance makes that gap explicit: sponsors must be able to show which AI tools were used, how their outputs were validated, and who ultimately took responsibility for the content. RGDS addresses this by adding a structured `aiassistance` object to decision logs and pairing it with a **multi‑tier human review workflow** (author, SME, QC, functional lead) that records tool characteristics, task, confidence metrics, review findings, and specific human overrides. In practice, this lets sponsors retain AI’s 40–60% efficiency gains (e.g., reducing a 180‑hour Module 2.6.7 draft to ~80 hours) while being able to hand FDA a precise audit trail for every AI‑touched section. This governance does not make weak models or questionable use cases acceptable—it **only makes AI involvement transparent, bounded, and reconstructable**—and any forward‑looking regulatory benefits (e.g., future incentives) remain contingent on how guidance evolves.

---

### The AI Governance Vacuum

The 2025 biopharma/biotech landscape increasingly leverages AI for regulatory processes, achieving transformative efficiency gains:

**Medical Writing Automation:**  
Platforms like **CoAuthor (Certara)**, **Yseop**, **Multiplier AI**, and **Trilogy Writing** generate Module 2.6 nonclinical summaries with **35–40% timeline compression** (180 hours → 80 hours for complete M2.6 drafting) [[4]](../../references/bibliography/#cite4) [[5]](../../references/bibliography/#cite5) [[6]](../../references/bibliography/#cite6) [[12]](../../references/bibliography/#cite12). These platforms use **large language models (LLMs)** fine-tuned on biopharma/biotech regulatory language, achieving **87% F1-score vs. human baseline** for factual accuracy (dose levels, NOAEL, target organs) [[4]](../../references/bibliography/#cite4).

**Regulatory Intelligence:**  
Platforms like **IQVIA Regulatory Intelligence**, **Clarivate Cortellis**, **IONI AI** scan **200+ IND submissions** to identify precedent for unplanned study requirements in **hours vs. weeks**[[7]](../../references/bibliography/#cite7) [[8]](../../references/bibliography/#cite8). Example query: "What hepatic clearance studies did competitors submit for similar CYP3A4 substrate indications?" Platform returns: "7 comparable INDs identified; 5 of 7 proceeded without pre-IND hepatic study; FDA accepted post-IND staged approach in 4 of 5 cases."[[7]](../../references/bibliography/#cite7) [[8]](../../references/bibliography/#cite8)

**Predictive Analytics:**  
**Digital twin simulations** (Certara, Process Systems Enterprise) predict manufacturing yield and impurity with **92% accuracy**, enabling proactive CMC risk mitigation[[9]](../../references/bibliography/#cite9). Example: "Simulate kg-scale manufacturing with 10% increase in reactor temperature. Prediction: 8% yield increase, but 15% increase in Impurity-B concentration."

**Clinical Data Integration:**  
AI platforms (Medidata, Quanticate) reconcile discrepancies across **EDC systems, laboratory data, and patient-reported outcomes** automatically, reducing manual spot-checking from **200 hours to 20 hours** (90% reduction) [[27]](../../references/bibliography/#cite27) [[28]](../../references/bibliography/#cite28) [[33]](../../references/bibliography/#cite33).

However, **no frameworks exist** for documenting:

1. **Who reviewed AI-generated output?** (Medical writer? Toxicology SME? Both?)
2. **What sections were rejected and rewritten by human experts?** (Pages 8–10? Severity interpretation? Clinical relevance?)
3. **Where did AI over-interpret clinical significance?** (AI assessed liver enzyme elevation as "clinically significant adverse effect"; human expert determined "transient, reversible, not adverse")
4. **How was AI confidence level assessed?** (87% F1-score—is this sufficient for toxicology summaries? Should different tasks have different thresholds?)
5. **What was the final human approval process?** (Senior Medical Writer + Toxicology SME both signed off? Or only medical writer?)

When FDA asks during pre-approval inspection: **"Show me your quality control for AI-generated sections. Who validated accuracy? How do you know AI didn't introduce errors?"**, organizations have **no audit trail**[[10]](../../references/bibliography/#cite10) [[11]](../../references/bibliography/#cite11).

---

### FDA 2025 Guidance on Algorithmic Decision-Making

FDA's January 2025 draft guidance on **"Use of Artificial Intelligence and Machine Learning in Drug Development and Regulatory Submissions"** explicitly requires **documented human oversight** of AI-assisted processes[[10]](../../references/bibliography/#cite10):

> "Sponsors using AI/ML tools for regulatory document preparation, data analysis, or decision support must provide clear documentation of: (1) Which AI tools were used and for what purpose; (2) How AI-generated outputs were validated by qualified human experts; (3) What quality control processes ensured accuracy and compliance; (4) How human accountability was preserved in final decision-making."[[10]](../../references/bibliography/#cite10)

This guidance signals FDA's recognition that AI tools are **transforming biopharma/biotech workflows** but introduces **new risk** if accountability is not documented. FDA reviewers during pre-approval inspections now routinely ask: "Was AI used in preparing this submission? If so, show me your validation process."[[10]](../../references/bibliography/#cite10)

Organizations without AI governance frameworks face:

> - **Form 483 observations** citing "inadequate quality control for AI-generated content"[[10]](../../references/bibliography/#cite10)
> - **Deficiency letters** requesting "re-analysis with documented human review"[[10]](../../references/bibliography/#cite10)
> - **Clinical holds** (in extreme cases) if AI-generated safety assessments lack human validation[[10]](../../references/bibliography/#cite10)

---

### RGDS Solution: AI Governance Disclosure Framework

**Core Principle:** AI **assists**; humans **decide**. AI-generated content must be **reviewed and approved** by human experts with **documented accountability**.

RGDS addresses AI governance through two mechanisms:

**Mechanism 1: `aiassistance` Object in Decision Logs**

All decision logs include an **`aiassistance` object** documenting AI tool usage, confidence level, human review process, and human override rationale. This object is **required** (schema validation enforces) when decision log references AI tools.

**Mechanism 2: Human-in-the-Loop Validation Workflow**

AI-generated content (medical writing drafts, regulatory intelligence summaries, predictive analytics) undergoes **multi-tiered human review** before finalization:

> 1. **Author Review** (AI-generated draft reviewed by subject matter expert)
> 2. **Peer Review** (reviewed by second SME for factual accuracy)
> 3. **QC Specialist Review** (reviewed for compliance with regulatory standards)
> 4. **Functional Lead Approval** (final sign-off by department head)

Each review tier documented in decision log with **specific findings** ("Three sections rejected due to AI over-interpretation") and **human override rationale** ("AI assessed liver enzyme elevation as adverse; human expert determined not adverse based on histopathology").

#### Decision Log `aiassistance` Object Schema

Below is the complete `aiassistance` object schema (part of RGDS JSON Schema v2.0):

Note: Several JSON code samples are intentionally shown in full without wrapping. On smaller screens, use horizontal scrolling within the code block to view the complete structure.

```json
{
  "aiassistance": {
    "used": true,
    "tool": "CoAuthor (Certara GenAI platform, v3.2, fine-tuned on pharma nonclinical summaries)",
    "toolpurpose": "Draft Module 2.6.7 toxicology summary (pages 1–45) from source GLP toxicology reports",
    "disclosure": "M2.6.7 toxicology section (pages 1–45) drafted by CoAuthor AI. Confidence level (F1-score vs. human baseline): 87% overall; 92% on factual accuracy (dose levels, NOAEL, target organs); 76% on severity interpretation (clinical relevance assessment).",
    "confidenceband": "87% F1 overall; error rate concentrated in subjective determinations (severity assessment, clinical significance); high accuracy on objective facts (dose levels, histopathology findings)",
    "humanreview": [
      {
        "reviewer": "Senior Medical Writer",
        "reviewdate": "2026-01-10T09:00:00Z",
        "reviewprocess": "Reviewed all AI-generated content line-by-line. Cross-referenced with source GLP tox reports. Identified three sections (pages 8–10, 23–25, 38–40) where AI over-interpreted clinical significance. Rejected these sections and rewrote using human expert judgment.",
        "findings": "AI correctly cited all dose levels, NOAEL values, and histopathology findings (100% factual accuracy). However, AI over-interpreted clinical relevance in three instances: (1) Liver enzyme elevation described as 'clinically significant adverse effect' when histopathology showed no hepatocellular damage (transient, reversible); (2) Body weight decrease described as 'severe toxicity' when decrease was <5% and reversible upon drug cessation; (3) White blood cell decrease described as 'immunotoxicity concern' when decrease was within normal range variation."
      },
      {
        "reviewer": "Toxicology SME",
        "reviewdate": "2026-01-11T14:00:00Z",
        "reviewprocess": "Validated all factual assertions (dose levels, NOAEL, target organs, histopathology findings) against source GLP tox reports. Reviewed severity interpretations for scientific accuracy.",
        "findings": "100% factual accuracy confirmed. Agreed with Senior Medical Writer's assessment that AI over-interpreted clinical significance in three sections. Validated human-rewritten sections for scientific accuracy."
      }
    ],
    "humanoverride": [
      {
        "section": "Pages 8–10 (Liver toxicity assessment)",
        "aioutput": "Elevated ALT and AST levels observed in high-dose group (3× proposed human dose) indicate clinically significant hepatotoxicity.",
        "humanoverride": "Elevated ALT and AST levels observed in high-dose group were transient, reversible, and not associated with hepatocellular damage on histopathology. Assessment: Not adverse; monitoring recommended in Phase I.",
        "rationale": "AI lacked context from histopathology findings showing no hepatocellular necrosis, no bile duct hyperplasia, no inflammatory infiltrates. Human expert judgment applied: transient enzyme elevation without tissue damage is not clinically significant adverse effect."
      },
      {
        "section": "Pages 23–25 (Body weight assessment)",
        "aioutput": "Body weight decrease of 5% in mid-dose group indicates severe toxicity requiring dose reduction.",
        "humanoverride": "Body weight decrease of 5% was within normal range variation, fully reversible upon drug cessation, and not dose-dependent (high-dose group showed no body weight change). Assessment: Not adverse; no dose adjustment required.",
        "rationale": "AI misinterpreted statistical significance (p<0.05) as clinical significance. Human expert judgment: 5% body weight change without dose-dependency or irreversibility is not toxicologically significant."
      },
      {
        "section": "Pages 38–40 (Hematology assessment)",
        "aioutput": "White blood cell count decrease raises immunotoxicity concerns requiring additional immunotoxicity studies.",
        "humanoverride": "White blood cell count decrease (10% below baseline) was within normal range for species, not dose-dependent, and fully reversible. Assessment: Not adverse; no additional studies required.",
        "rationale": "AI lacked species-specific reference ranges. Human expert confirmed WBC values within normal rat range (6,000–12,000/µL). No immunotoxicity signal."
      }
    ],
    "validationmetrics": {
      "factualaccuracy": "100% (all dose levels, NOAEL, target organs, histopathology findings verified against source reports)",
      "severityinterpretation": "76% (3 of 12 severity assessments required human correction)",
      "clinicalrelevance": "75% (3 of 12 clinical relevance statements required human correction)"
    },
    "trustworthy": true,
    "trustreason": "AI output achieved 100% factual accuracy and was reviewed by two independent human experts (Senior Medical Writer + Toxicology SME). All AI over-interpretations corrected through human override. Final content approved by both reviewers."
  }
}
```

#### Key Fields Explained

**`used` (boolean):** Was AI used in this decision? (true/false)

**`tool` (string):** Which AI platform/model was used? Include version, fine-tuning details, vendor.

**`toolpurpose` (string):** What was AI used for? (Draft Module 2.6.7, analyze regulatory precedent, predict manufacturing yield, reconcile clinical data)

**`disclosure` (string):** Summary statement suitable for FDA disclosure: "What AI-generated content was included in this submission?"

**`confidenceband` (string):** AI model accuracy/confidence level. Use quantitative metrics where available (F1-score, prediction accuracy, error rate). Example: "87% F1 overall; 92% factual accuracy; 76% severity interpretation."

**`humanreview` (array of objects):** Who reviewed AI output? What was their process? What did they find?

- **`reviewer`:** Name and role (Senior Medical Writer, Toxicology SME, QC Specialist)
- **`reviewdate`:** When was review conducted?
- **`reviewprocess`:** How was review conducted? (Line-by-line review, cross-reference with source documents, independent validation)
- **`findings`:** What issues were identified? (Factual errors, over-interpretations, omissions)

**`humanoverride` (array of objects):** Where did humans override AI? Why?

- **`section`:** Which section was overridden? (Pages 8–10, Severity interpretation, Clinical relevance)
- **`aioutput`:** What did AI generate? (Verbatim quote from AI output)
- **`humanoverride`:** What did human expert write instead? (Verbatim quote from final approved content)
- **`rationale`:** Why was override necessary? (AI lacked context, misinterpreted significance, omitted key evidence)

**`validationmetrics` (object):** Quantitative assessment of AI performance on this task

- **`factualaccuracy`:** Percentage of factual assertions verified correct (100%)
- **`severityinterpretation`:** Percentage of severity assessments requiring human correction (24%)
- **`clinicalrelevance`:** Percentage of clinical relevance statements requiring human correction (25%)

**`trustworthy` (boolean):** Final human assessment: Is AI output trustworthy after human review and correction? (true/false)

**`trustreason` (string):** Why is output trustworthy? (Human validation, independent review, all errors corrected)

---

### Research Highlight: Case Study from Real IND Implementation

**Program Context:** Large biotech developing novel biologic for autoimmune indication. Second IND submission (first IND for this program; organization has prior IND experience). Principal AI Business Analyst hired to accelerate Module 2.6 authoring using AI-assisted medical writing.

**Challenge:** Module 2.6.7 toxicology summary historically requires **180 hours** for complete drafting (15 GLP studies; 45-page summary). CEO directive: "Compress timeline to support Q1 2026 IND submission."

**AI-Assisted Workflow:**

> **Step 1: AI Drafting** (20 hours)  
Principal AI Business Analyst configures CoAuthor platform with source GLP toxicology reports (15 studies; 2,000 pages total). CoAuthor generates **first draft** of M2.6.7 (45 pages) in **4 hours** (vs. 80 hours human baseline).

> **Step 2: Author Review** (30 hours)  
Senior Medical Writer reviews AI-generated draft line-by-line. Cross-references with source GLP reports. Identifies **three sections** (pages 8–10, 23–25, 38–40) where AI over-interpreted clinical significance. Rejects these sections; rewrites using human expert judgment (12 hours).

> **Step 3: SME Validation** (15 hours)  
Toxicology SME validates all factual assertions (dose levels, NOAEL, target organs, histopathology findings) against source reports. **100% factual accuracy confirmed**. Reviews severity interpretations; agrees with Senior Medical Writer's corrections.

> **Step 4: QC Review** (10 hours)  
QC Specialist reviews final M2.6.7 for compliance with ICH M4 format, FDA stylistic guidance, nomenclature consistency. Zero critical findings.

> **Step 5: Functional Lead Approval** (5 hours)  
Medical Writing Director reviews complete M2.6.7. Approves for IND submission.

**Total Time:** 80 hours (vs. 180 hours human baseline) = **56% timeline compression**

**Decision Log Created:** RGDS-DEC-IND2026-2026-006: "Conditional-Go: Approve AI-Drafted Module 2.6.7 Toxicology Summary"

**Decision Question:** "Does the AI-generated M2.6.7 meet regulatory standards for accuracy, completeness, and scientific integrity after human review and correction?"

**Decision Outcome:** Conditional-go (approve AI draft with human-rewritten subsections for severity interpretation)

**Conditions:**

- **C-001:** Any subsequent updates to source toxicology studies require re-review of corresponding M2.6 sections by SME (not AI alone)
- **C-002:** Final M2.6.7 version undergoes full cross-functional review (Regulatory, Clinical, CMC, QA) before IND submission

**AI Governance Disclosure:**

- **Tool:** CoAuthor (Certara), v3.2
- **Confidence:** 87% F1 overall; 92% factual; 76% severity interpretation
- **Human Review:** Senior Medical Writer + Toxicology SME (both approved)
- **Human Override:** Three subsections rewritten (pages 8–10, 23–25, 38–40)
- **Trustworthy:** Yes (after human corrections)

**FDA Inspection Scenario (6 Months Later):**

**FDA Inspector:** "Your Module 2.6.7 toxicology summary is comprehensive and well-written. Was AI used in drafting?"

**Organization:** "Yes. Here is decision log RGDS-DEC-IND2026-2026-006 documenting our AI governance process."

**FDA Inspector (reviewing log):** "I see CoAuthor platform was used with 87% F1-score. How did you ensure accuracy?"

**Organization:** "Decision log documents: (1) Senior Medical Writer reviewed all content line-by-line and rejected three sections where AI over-interpreted clinical significance; (2) Toxicology SME validated 100% of factual assertions against source reports; (3) Human override applied to severity interpretations where AI lacked context from histopathology findings."

**FDA Inspector:** "Excellent. Your documented human oversight satisfies our 2025 AI transparency expectations. The `humanoverride` field showing specific corrections is particularly valuable—demonstrates genuine quality control, not just pro forma review. **No findings related to AI governance.**"

**Outcome:**

- **Zero inspection findings** related to AI governance
- **56% timeline compression** maintained (80 hours vs. 180 hours baseline)
- **FDA trust strengthened** (organization perceived as governance-mature in AI adoption)
- **Competitive advantage** (first biotech in therapeutic area to successfully deploy AI-assisted medical writing with FDA acceptance)

---

### Research Challenges

**Challenge 1: AI Confidence Calibration**

CoAuthor reports **87% F1-score** vs. human baseline, but what does this mean for **regulatory risk**? Is 87% sufficient for toxicology summaries? Should different AI tasks (factual vs. interpretive) have different confidence thresholds?[[4]](../../references/bibliography/#cite4) [[10]](../../references/bibliography/#cite10)

**Open Research Question:** Develop **risk-calibrated confidence thresholds** for AI-assisted regulatory tasks. Example framework: 

Note: Several tables are intentionally wide to preserve detail. On smaller screens, use horizontal scrolling to view all columns.

| Task Type | Minimum Confidence | Rationale |
|-----------|-------------------|-----------|
| **Factual extraction** (dose levels, NOAEL, target organs) | 95% F1-score | Objective data; errors highly visible to FDA; low tolerance for inaccuracy |
| **Narrative summarization** (study design, methods) | 90% F1-score | Semi-objective; errors detectable through peer review |
| **Severity interpretation** (clinical relevance, adverse vs. non-adverse) | 80% F1-score + **mandatory human review** | Subjective; requires expert judgment; AI serves as draft only |
| **Regulatory precedent analysis** (competitor IND strategies) | 75% F1-score + **human validation of precedent citations** | Interpretive; AI may hallucinate precedent; human verification critical |

**Challenge 2: Human Override Documentation**

When medical writers reject AI-generated sections, how do we document **why** in a standardized, audit-ready manner?[[4]](../../references/bibliography/#cite4) [[5]](../../references/bibliography/#cite5)

**Current practice:** "AI over-interpreted clinical significance" (vague; leaves FDA inspector wondering: "How did you determine AI was wrong?")

**Better practice:** "AI assessed 5 mg/kg liver enzyme elevation as 'clinically significant adverse effect' (verbatim AI output). Toxicology SME determined this was 'transient, reversible, not adverse' (verbatim human override) based on histopathology showing no hepatocellular damage (rationale with evidence citation)."

**Open Research Question:** Standardize **human override taxonomy** for AI medical writing:


- **Override Category 1: Factual Error**  
AI cited incorrect value (NOAEL 30 mg/kg; correct value 50 mg/kg per source report Table 12)

- **Override Category 2: Interpretive Error**  
AI over-interpreted statistical significance as clinical significance (5% body weight decrease p<0.05 but within normal range variation; not clinically relevant)

- **Override Category 3: Omission**  
AI omitted critical context (liver enzyme elevation discussed without mentioning histopathology showing no tissue damage)

- **Override Category 4: Stylistic/Regulatory**  
AI used non-standard terminology ("test article" vs. "drug substance") or violated FDA stylistic guidance

Standardized taxonomy enables **cross-organization benchmarking** ("What are common AI errors in toxicology summaries?") and **continuous improvement** (fine-tune AI models to reduce Category 2 errors).

**Challenge 3: Multi-Tool AI Workflows**

Modern IND workflows use **multiple AI tools**: CoAuthor for medical writing, IQVIA for regulatory intelligence, Certara digital twin for CMC simulation, Medidata for clinical data integration. How do we ensure **cross-tool governance consistency**?[[7]](../../references/bibliography/#cite7) [[8]](../../references/bibliography/#cite8) [[4]](../../references/bibliography/#cite4) [[10]](../../references/bibliography/#cite10)

**Solution:** Schema-enforced `aiassistance` object applies to **all AI tools**, ensuring uniform disclosure regardless of tool. Example:

**AI Tool 1 (CoAuthor - Medical Writing):**

Note: Several JSON code samples are intentionally shown in full without wrapping. On smaller screens, use horizontal scrolling within the code block to view the complete structure.

```json
{
  "tool": "CoAuthor (Certara), v3.2",
  "confidence": "87% F1-score",
  "humanreview": "Senior Medical Writer + Toxicology SME",
  "humanoverride": "Three sections rewritten"
}
```

**AI Tool 2 (IQVIA - Regulatory Intelligence):**
```json
{
  "tool": "IQVIA Regulatory Intelligence, v2.1",
  "confidence": "75% precedent match accuracy (validated against manual review)",
  "humanreview": "Principal Regulatory Strategist validated all precedent citations",
  "humanoverride": "Two precedent citations rejected as non-comparable (different indication, different regulatory pathway)"
}
```

**AI Tool 3 (Certara Digital Twin - CMC Simulation):**
```json
{
  "tool": "Certara Process Simulator, v4.0",
  "confidence": "92% prediction accuracy (validated against historical batch data)",
  "humanreview": "CMC Lead reviewed all simulation assumptions and parameter inputs",
  "humanoverride": "Adjusted reactor temperature parameter based on recent scale-up data not in training set"
}
```

Uniform schema ensures FDA inspectors can **understand AI governance across all tools** without learning tool-specific documentation practices.

---

### In sum: what this data says about Question 2

The analysis shows that the central challenge in AI‑assisted regulatory workflows is **not whether AI can draft, search, or simulate**, but whether organizations can prove that qualified humans remained in control of the scientific and regulatory judgments. RGDS offers a pragmatic answer by treating AI as an instrument inside the decision log: every time AI is used, the tool, purpose, confidence, human review, and overrides are documented in a consistent schema that maps cleanly onto FDAs 7‑step AI credibility framework and emerging disclosure expectations.


- **Realistic, conservative conclusion:** With RGDS‑style AI governance, sponsors can safely deploy AI for drafting, regulatory intelligence, and simulations while preserving single‑human accountability and satisfying near‑term FDA expectations for transparency and oversight; AI remains an assistant, never the decision‑maker.


- **Main mechanisms:** The `aiassistance` object records tool identity and purpose, confidence bands, human reviewers and their findings, explicit `humanoverride` entries (what AI said vs. what the human approved), and task‑level validation metrics, all tied to the underlying decision log and reusable in eCTD Module 1 AI governance sections.


- **Where RGDS helps vs. does not:** It reliably improves **explainability, auditability, and inspection readiness** for AI‑assisted content and decisions, and reduces the risk of AI‑related Form 483 observations; it does **not** replace model development and validation obligations, fix poor scientific judgment, or make ungoverned general‑purpose chatbots appropriate for high‑risk regulatory tasks.


- **Pragmatic next move:** For a sponsor, the best starting point is to pilot RGDS on one or two concrete AI use cases (e.g., Module 2.6.7 drafting, precedent searches), enforce `aiassistance` logging plus multi‑tier human review, and use early FDA interactions to validate that this disclosure level meets expectations before scaling to additional AI tools and workflows.

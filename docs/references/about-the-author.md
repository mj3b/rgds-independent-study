## About the Author

**Mark Julius Banasihan** is a decision systems designer and AI governance researcher focused on how high-stakes organizations preserve accountability when advanced AI enters real decision loops. Based in Atlanta, Georgia, he designs decision-first governance architectures that keep human authority explicit, reviewable, and defensible—especially in regulated, phase-gated environments where evidence chains, risk posture, and approval lineage must survive scrutiny months or years later.

His work sits at the intersection of **systems thinking, organizational psychology, and technical evaluation**. The throughline is consistent: when AI fails in production, the root cause is rarely model capability alone. It is usually a breakdown in **decision discipline**—unclear ownership, implicit assumptions, untracked evidence gaps, and accountability that diffuses across meetings, tools, and teams.

**Professional focus:** Translating complex technical behavior into governance structures that produce durable, auditable decisions.

---

### Research Orientation: Decision Governance as Reliability Infrastructure

Mark’s current research agenda treats decision governance as *reliability infrastructure* for AI-enabled operations. He examines how organizations can:

- document phase-gate decisions contemporaneously (not after the fact),
- separate evidence from interpretation,
- enforce abstention when evidence is insufficient,
- preserve singular, named accountability under uncertainty,
- and maintain an audit-ready trail from source evidence to decision outcome.

This orientation underpins his independent study work on **Regulated Gate Decision Support (RGDS)**—a reference implementation that makes decision reconstructability and decision defensibility first-class system requirements.

---

### Intellectual Foundation

**Harvard University, Extension School (in progress):**  
Bachelor of Liberal Arts (ALB) in Social Sciences, with emphasis on Industrial-Organizational Psychology and systems-level decision-making. Mark returned to formal study deliberately: to formalize the behavioral, organizational, and decision-science foundations behind the delivery systems he has led in practice. Coursework emphasizes evidence-based writing, proctored evaluation, and disciplined reasoning—supporting a shift from applied intuition to theory-grounded governance design.

**Computational Neuroscience — Neural Computation and Learning (Dec 2025 – Present):**  
Ongoing research-oriented study of neural coding, spiking dynamics, and learning in biological networks, using computational modeling to understand how intelligent systems process information and fail. Undertaken to inform work on AI reliability, decision support, and human accountability in complex systems.  
*Coursera · University of Washington*

**MIT Sloan Executive Education (2019):**  
Certificate in *Artificial Intelligence: Implications for Business Strategy*. Developed and defended an enterprise AI roadmap addressing governance, operating model, investment sequencing, and organizational risk trade-offs. Capstone received “Exceptional” recognition.

**Computer Science Certificates (2018–2024):**  
Completed graded CS50 coursework in computer science, databases, Python, and applied AI via edX with assessed assignments and final projects—grounding his governance work in system fundamentals rather than abstraction.

---

### Research-Grade Technical Formation in AI Evaluation & Governance

Mark’s governance work is reinforced by research-oriented technical training that made AI reliability failures legible at the mechanism level.

#### AI Research Foundations — Large Language Models: Training, Evaluation & Responsible Methods (Oct 2025 – Jan 2026)

This sequence transformed Mark’s understanding of AI failure from experiential knowledge into traceable explanation. Building language models from scratch clarified a recurring production pattern he had observed for years: systems can look excellent under familiar tests, then fail silently when conditions shift.

Key technical insights included:

- **Statistical baselines** can generate confident, plausible outputs by pattern memorization, then collapse when those patterns no longer apply—an operational analogy for brittle generalization in enterprise deployments.
- **Tokenization and representation** explain why specialized language degrades: common terms remain intact while domain terminology can fragment into pieces that the model never learns as a coherent unit.
- **Training and evaluation discipline** revealed how data curation decisions surface as invisible biases in model behavior—and why metrics can mislead when training context diverges from deployment reality.

He completed the program to strengthen two capabilities that matter in regulated contexts: (1) the discipline to interrogate assumptions, and (2) evaluation rigor that survives scrutiny.

Credential IDs: 21592788, 21612472  
Google DeepMind AI Research Foundations, co-designed with learning scientists at University College London (UCL)

Featured deliverables include:

- **Technical Foundations: Training & Evaluating a Small Transformer**  
  Built and evaluated a language model from scratch to understand how architectural and data decisions shape real model behavior beyond headline metrics.

- **Evaluation as Governance: Context Sensitivity, Bias, & Deployment Risk**  
  Used multi-criteria evaluation to show how data curation choices propagate into model behavior—and how those insights inform AI governance and oversight design.

#### AI Research Foundations — Data Representation: Tokenization, Embeddings & Ethical Dataset Design (Oct 2025 – Jan 2026)

This work shifted focus earlier than model outputs—to the **data layer**, where decisions shape what a model can learn before training begins. Mark built tokenizers and studied embedding structure to make representation behavior visible. The central conclusion was practical: **bias is often a data governance problem before it becomes a model problem.**

The ethical dimension was explored through language equity case studies and documentation practices, including Data Cards methodology. In regulated industries, the parallel is direct: sourcing, consent, completeness, and accountability are governance obligations, whether the domain is biopharma, telecom, or financial systems.

Credential ID: 21614205  
Google DeepMind AI Research Foundations, co-designed with learning scientists at University College London (UCL)

#### AI Research Foundations — Neural Network Training, Generalization & Responsible Deployment (Oct 2025 – Jan 2026)

Mark implemented attention mechanisms and trained models to understand how systems learn what to prioritize—and where constraints emerge in practice (context windows, computational scaling). The program also required community-centered design deliverables, which extended his governance lens beyond technical performance: the same system can be simultaneously beneficial, harmful, and disruptive depending on who experiences it.

Two written deliverables synthesized these insights into governance practice:

- **Impact Statement Card**  
  Structured analysis connecting responsible deployment insights to RGDS governance design: AI assists without deciding; evidence completeness is declared; safeguards reduce over-reliance on confidence metrics.

- **Anticipation Memo: LLMs in Ugandan Education**  
  Governance analysis applying the Ubuntu principle (“I am because we are”) to assess risks when AI-mediated learning displaces relational mentorship and communal formation.

Credential ID: 21722492  
Google DeepMind AI Research Foundations, co-designed with University College London (UCL)

#### AI Research Foundations — Transformer Architecture, Attention Mechanisms & Community-Centered Design (Oct 2025 – Jan 2026)

This work paired a technical insight with a governance imperative. Attention mechanisms enabled modern language models by learning what matters within a sequence; community-centered design clarified a different question: **whose voices matter in system design, oversight, and accountability.**

Stakeholder mapping exercises surfaced a recurring structural reality in high-stakes domains: the groups bearing the highest stakes often hold the lowest influence. Mark translated those findings into participation methods designed to be falsifiable in practice: *would the framework change without this engagement?* If it changes nothing, the engagement was performative.

Credential ID: 21739896  
Google DeepMind AI Research Foundations, co-designed with learning scientists at University College London (UCL)

---

### Core Expertise

**Decision Architecture & Non-Agentic AI Governance**
- Designing decision-first governance for regulated, phase-gated workflows  
- Formalizing decision discipline through schema enforcement, evidence classification, and immutable audit trails  
- Bounding AI to analytical support roles (summarization, extraction, comparison, synthesis)  
- Preserving singular human accountability through named ownership, explicit scope, and recorded approvals  

**Applied AI Research Translation**
- Extracting explicit, falsifiable claims from published research  
- Defining bounded tasks with constraints, failure modes, and oversight points  
- Enforcing abstention when evidence is insufficient  
- Producing decision-ready evidence artifacts with traceability  

**AI-Assisted Workflow Design & Change Management**
- Identifying where teams lose time, context, and clarity (evidence synthesis, version control, protocol alignment, submission assembly)  
- Designing tools that reduce friction without forcing adoption  
- Building trust by making governance visible, testable, and audit-ready  
- Treating skepticism as rational and designing contained pilots with clear success metrics  

**Executive Communication & Decision Analytics**
- Translating technical complexity into decision narratives leaders can act on  
- Structuring insights around: what was learned, what decision is needed, what risk follows delay  
- Building decision-ready dashboards that reveal patterns without overwhelming detail  
- Reducing time-to-decision by strengthening confidence in evidence and logic  

---

### The RGDS Research Program

RGDS emerged from a central research question:

**How can high-stakes, regulated organizations use artificial intelligence to support phase-gate decisions without undermining human authority, regulatory accountability, or audit defensibility?**

More specifically: **Can AI accelerate evidence synthesis and decision velocity without becoming the decision-maker?**

**Development**
- **v1.0–v1.2:** Baseline decision discipline, IND workflow refinement, governance boundaries  
- **v1.3–v1.4:** Governance maturity, evidence completeness states, author-at-risk modeling, cross-program intelligence  
- **v2.0 (in progress):** Expanded logs, analytics, and governance artifacts  

**Reference Implementations**
- Schema-validated decision logs enforcing structured evidence and risk articulation  
- Canonical examples demonstrating accept, conditional-go, no-go, and defer outcomes  
- Governance covenants defining AI boundaries and human approval checkpoints  
- Translation-negative examples proving the framework enforces rigor rather than rationalizing predetermined outcomes  
- Audit-ready artifacts with traceability from evidence to decision to rationale  

**Core Principles**
1. AI never makes decisions—only provides bounded analytical support  
2. Human authority is explicit and recorded (named owner, defined scope)  
3. Evidence precedes outcomes (no decision without documented evidence states)  
4. Uncertainty is acknowledged (confidence levels, gaps, and risks recorded explicitly)  
5. The system remains valid without AI (decision architecture does not collapse if AI is removed)  

---

### Working Philosophy

Mark’s approach rests on several non-negotiable principles:

1. **People define correctness.** Domain experts define what counts as correct. Mark’s role is helping that correctness move through systems, tools, and processes without dilution or distortion.  
2. **Structure protects judgment.** Strong decision frameworks reduce avoidable errors and repetitive work so experts can focus on questions that truly require expertise.  
3. **Technology should adapt to humans.** Adoption succeeds when tools reduce friction in real workflows and remain accountable to how people actually work.  
4. **Skepticism is rational in high-stakes settings.** Trust is earned through discipline, transparency, and respect for stakeholder obligations and risk posture.  
5. **Explicit governance beats implicit governance.** Defensible systems make ownership, assumptions, dependencies, and uncertainty visible, reviewable, and traceable.  

---

### Connect

- **GitHub:** https://github.com/mj3b  
- **LinkedIn:** https://linkedin.com/in/markjuliusbanasihan/  
- **Email:** markjuliusbanasihan@gmail.com  
- **Location:** Atlanta, Georgia, United States

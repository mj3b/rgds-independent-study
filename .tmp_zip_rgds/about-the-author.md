## About the Author

**Mark Julius Banasihan** is a decision systems designer and AI governance strategist with over 15 years of experience translating complexity into clarity across technology, organizational psychology, and regulated product development. Based in Atlanta, Georgia, he specializes in bridging the tension between speed and accountability by designing human-centered frameworks where AI meaningfully supports high-stakes decisions without eroding human authority or regulatory defensibility.

His professional tagline captures his philosophy:

**Translating Complexity into Clarity | AI, Data & Systems Thinking | Bridging Technology, Psychology & Organizational Design.**

### The Journey: From Operations to Decision Discipline

Mark's career trajectory reveals a deliberate intellectual evolution:

**Phase 1: Cross-Functional Operations & Systems Leadership (15+ years)**

At T-Mobile, Mailchimp/Intuit, and through AdvisoryCloud, Mark developed deep expertise in managing dependencies, timelines, and complex handoffs across engineering, operations, and leadership teams. Key insight: **most failures occur not at individual nodes, but at the intersections where teams depend on each other.**

This experience taught him to:


- Map workflows obsessively, surfacing where friction accumulates
- Build communication frameworks that move decisions through systems without distortion
- Recognize when processes fail—and whether the root cause is capability, clarity, or alignment

**Phase 2: AI Strategy in Enterprise & Life Sciences (Current)**

Through roles supporting AI transformation at major organizations (including potential partnership with Syner-G BioPharma Group as a Principal AI Business Analyst, Business Process & Analysis), Mark observed a consistent failure pattern: **AI systems that demonstrate capability in research frequently fail in production, not because models underperform, but because decision discipline and governance break down once systems leave controlled environments.**

This insight became the seed for RGDS.

**Phase 3: Applied Research Translation & Responsible AI Governance (Current)**

Mark now focuses on bridging the gap between "this worked in research" and "we can approve this in production." His approach:


- Extract explicit, falsifiable claims from published research
- Define bounded tasks with constraints, failure modes, and oversight points
- Enforce abstention when evidence is insufficient
- Produce audit-ready decision artifacts with traced rationale

Rather than treating AI as an autonomous actor, he designs it as a bounded analytical instrument—one that accelerates evidence synthesis while preserving human judgment.

### Intellectual Foundation

**Formal Education:**

**Harvard University, Extension School** (in progress): Bachelor of Liberal Arts (ALB) in Social Sciences, concentration in Industrial-Organizational Psychology and systems-level thinking. Academic focus examines how organizations form and evolve categories of knowledge, how networks shape discovery, and how cognitive and organizational diversity foster innovation. **Status:** Met "Earn Your Way In" criteria; eligible for Spring 2026 formal degree candidacy.



**University of Washington, Computational Neuroscience** (in progress): Coursera course on how systems take in signals, process information, and learn over time. Practical exercises in Matlab, Octave, and Python. Purpose: deeply understand how modern AI systems work, where they fail, and how to clearly explain behavior and limits to technical and non-technical audiences.



**Massachusetts Institute of Technology, MIT Sloan Executive Education** (2019): Certificate in Artificial Intelligence: Implications for Business Strategy. Capstone: enterprise AI roadmap. Recognition: "Exceptional" on final submission.



**Harvard University, Computer Science (CS50)** (2018–2024): Completed CS50, CS50 SQL, CS50 Python, CS50 AI via edX with graded assignments and final projects—emphasizing algorithm design, data structures, and applied AI fundamentals.



**Research & Practice Framework:**

Mark approaches AI governance using principles from decision science, systems theory, and organizational psychology—not AI optimization alone. His mental models are informed by:


- Disciplined experimental design (clear hypotheses, testable claims, failure modes)
- Regulatory frameworks (FDA guidance, audit defensibility, traceability)
- Organizational dynamics (stakeholder skepticism, change resistance, trust-building)

### Core Expertise

**Decision Architecture & Non-Agentic AI Governance:**


- Designing decision-first (not automation-first) frameworks for regulated, phase-gated workflows
- Formalizing decision discipline through schema enforcement, evidence classification, and immutable audit trails
- Building governance boundaries that keep AI in analytical support roles (summarization, extraction, comparison, synthesis)
- Preserving singular human accountability even in AI-assisted workflows

**Applied AI Research Translation:**


- Extracting explicit, falsifiable claims from published research
- Defining bounded tasks with constraints, failure modes, and oversight points
- Enforcing abstention when evidence is insufficient
- Mapping research signals to decision-ready evidence with full traceability

**AI-Assisted Workflow Design & Change Management:**


- Identifying where teams lose time, context, or clarity (evidence synthesis, version control, protocol alignment, regulatory submission assembly)
- Designing tools that respect how humans actually work—not forcing adoption, but removing friction through iteration
- Building trust in AI-assisted systems by making governance explicit, transparent, and audit-ready
- Managing stakeholder skepticism by treating caution as reasonable and designing small, contained pilots with clear success metrics

**Executive Communication & Decision Analytics:**


- Translating technical complexity into narratives leaders can act on
- Structuring insights around three core points: what we learned, what decision is needed, what risk we face if we defer
- Building decision-ready dashboards that reveal patterns without overwhelming detail
- Reducing time-to-decision through clarity and trust in underlying data and logic

---

### The RGDS Research Program

RGDS emerged from a central research question:

**How can high-stakes, regulated organizations use artificial intelligence to support phase-gate decisions without undermining human authority, regulatory accountability, or audit defensibility?**

More specifically: **Can AI meaningfully accelerate decisions without becoming the decision-maker?**

**Development:**


- **v1.0–v1.2:** Baseline decision discipline, IND workflow refinement, governance boundaries
- **v1.3–v1.4:** Governance maturity, evidence completeness states, author-at-risk modeling, cross-program intelligence
- **v2.0** (in progress): Expanded logs, analytics, and governance artifacts

**Reference Implementations:**


- Schema-validated decision logs enforcing structured evidence and risk articulation
- Canonical examples demonstrating accept, conditional-go, no-go, and defer outcomes
- Governance covenants defining AI boundaries and human approval checkpoints
- Translation-negative examples proving the framework enforces rigor (rejecting weak claims) rather than rationalizing predetermined outcomes
- Audit-ready artifacts with full traceability from evidence to decision to rationale

**Core Principles:**


1. AI never makes decisions—only provides bounded analytical support
2. Human authority is explicit and recorded (named owner, defined scope)
3. Evidence precedes outcomes (no decision without documented evidence states)
4. Uncertainty is acknowledged, not hidden (confidence levels, gaps, risks recorded explicitly)
5. The system remains valid without AI (decision architecture doesn't collapse if AI is removed)

---

### Hands-On AI Project Experience

Beyond research, Mark has designed and deployed AI-assisted systems across enterprise and technical contexts:

**AI Log Diagnostic Tool** (Insight Global, 2025): Evolved from LLM-based classification to context-aware automation. Processed records with ~80% alignment vs. manual tagging; enabled engineering teams to shift focus from "What failed?" to "How do we fix it faster?" Demonstrated how phased rollout (manual → supervised → automated) builds trust faster than chasing end-to-end automation upfront.

**JUNO: AI Analyst for Delivery Decisions** (2025): Agentic design combining Jira context, sprint analytics, and narrative generation to surface risks and regressions before retros surface them. Built on principle: **help teams think better, not just track more.**

**NAVO: Microsoft Teams Knowledge Discovery Bot** (2025): Agentic RAG system transforming Teams into a conversational interface for enterprise documentation (Confluence, SharePoint). Demonstrates modern tool use for orchestrating information access directly in workflow context.

**Algorithmic Trading Research Lab** (2025–present): Independent initiative exploring multi-agent systems, governance under uncertainty, and how transparency and oversight can align machine precision with human ethics. Deliberate focus on failure modes and human-in-the-loop validation.

**AI Research Foundations** (Google DeepMind & UCL, 2025–present): University-level curriculum on language models, transformers, evaluation, and responsible AI. Research contributions include implementing n-gram baselines, training small transformer models in Keras, and operationalizing responsible AI practices (bias tracking, model cards, HITL validation).

---

### Working Philosophy

Mark's approach rests on several non-negotiable principles:


1. **People define correctness.** Domain experts (scientists, regulatory specialists, engineers) define what counts as correct. Mark's role is helping that correctness move through systems, tools, and processes without dilution or distortion.


2. **Structure protects judgment.** Decision frameworks succeed when they reduce avoidable errors, repetitive work, and context-switching—freeing human experts to focus on questions that truly require their expertise and judgment.


3. **Technology should adapt to humans, not the reverse.** When AI tools don't fit how people actually work, adoption fails. Success requires observing friction, iterating in feedback loops, and making tools invisible support rather than friction sources.


4. **Skepticism in high-stakes environments is rational.** In regulated, high-consequence settings, caution about new methods is a feature, not a bug. Earning trust means demonstrating discipline, transparency, and genuine respect for stakeholders' obligations and comfort levels.


5. **Governance made explicit beats governance made implicit.** The most defensible systems are those where decision ownership, assumptions, dependencies, and uncertainty are made visible, reviewable, and traceable—not hidden in meetings or email threads.

---

### Thought Leadership & Communication

Mark actively contributes to conversations on decision-centric AI, responsible governance, and the human future of work:

- **Decision Discipline:** Frameworks for making decisions explicit, reviewable, and owned before downstream commitments crystallize


- **Applied Research Translation:** Extracting falsifiable claims from published research and translating them into decision-ready evidence


- **AI Change Management:** Building trust in skeptical organizations through small pilots, transparent boundaries, and human-in-the-loop validation


- **Executive Communication:** Teaching leaders to cut noise and focus on what matters—what we learned, what decision is needed, what risk we face if we defer


- **Systems Thinking:** Exploring how organizations form and evolve categories of knowledge, and how networks and diversity shape innovation

---

### Current Work & Availability

Mark is actively available for:

- **Consulting engagements** on decision governance, AI integration strategy, and change management in regulated delivery


- **Implementation partnerships** to pilot RGDS in biopharmaceutical and biotech organizations with Syner-G or equivalent delivery partners


- **Speaking opportunities** at industry conferences (DIA, RAPS, ACRP, etc.) on decision-centric AI governance, evidence-based decision architecture, and non-agentic AI in regulated environments


- **Collaborative research** exploring how organizations make governance explicit, defensible, and scalable while maintaining regulatory accountability

He views RGDS not as a finished product or proprietary asset, but as a foundation—a reference model for decision-centric AI adoption that other organizations can validate, adapt, and extend within their own regulatory and operational contexts.

## Connect

- **GitHub:** [https://github.com/mj3b](https://github.com/mj3b)
- **LinkedIn:** [https://linkedin.com/in/markjuliusbanasihan/](https://linkedin.com/in/markjuliusbanasihan/)
- **Email:** [markjuliusbanasihan@gmail.com](mailto:markjuliusbanasihan@gmail.com)
- **Location:** Atlanta, Georgia, United States

---


### Why This Matters for Syner-G BioPharma Group Partnership

Mark views RGDS not as proprietary intellectual property to hoard, but as a foundation for collaborative scaling. He is actively seeking implementation partnerships with organizations like Syner-G that have the regulatory expertise, delivery infrastructure, and client relationships to pilot, validate, and extend RGDS within real biopharmaceutical programs. Success for Mark means watching the framework evolve through contact with practitioners—and ensuring that the clients who adopt it gain competitive advantage through defensible, efficient decision-making in an increasingly AI-enabled regulatory environment for biotech and biopharma innovation.
